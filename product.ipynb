{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectif du Notebook\n",
    "Dans ce notebook, notre objectif principal est de fusionner les différentes tables liées aux produits, qui sont stockées dans notre Data Warehouse. Cette fusion permettra de regrouper toutes les informations pertinentes sur les produits en un seul jeu de données cohérent et consolidé.\n",
    "\n",
    "En effet, ces tables contiennent des informations clés, telles que les catégories de produits, les sous-catégories, les régions de vente, ou encore les détails des commandes. En combinant ces données, nous simplifions leur structure pour qu'elles soient prêtes à être utilisées dans des algorithmes de machine learning. Cela facilitera des tâches telles que la prédiction des ventes, l'analyse des tendances, ou encore le regroupement des produits en fonction de leur performance.\n",
    "\n",
    "Ce processus de fusion est une étape essentielle dans le pipeline de préparation des données, car il garantit que toutes les relations entre les différentes entités du Data Warehouse sont prises en compte et intégrées dans le jeu de données final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion à la base de données\n",
    "\n",
    "Cette section configure et établit une connexion à la base de données SQL Server à l'aide de pyodbc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à la base de données AVW_DW !\n",
      "  YearKey  Year  IsDeleted\n",
      "0    2011  2011      False\n",
      "1    2012  2012      False\n",
      "2    2013  2013      False\n",
      "     Id   DateKey  TerritoryKey  SalePersonKey   Revenue  NumberOrder  \\\n",
      "0  7393  01102011             5              1   2417.47            1   \n",
      "1  7394  30102012             5              1  90167.33            1   \n",
      "2  7395  30082012             4              1  33206.02            1   \n",
      "3  7396  30082012             2              1  29638.27            1   \n",
      "4  7397  30062013             6              1  17990.96            1   \n",
      "\n",
      "   IsDeleted  \n",
      "0      False  \n",
      "1      False  \n",
      "2      False  \n",
      "3      False  \n",
      "4      False  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_year = pd.read_sql(\"SELECT * FROM Dim_Year\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:21: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_month = pd.read_sql(\"SELECT * FROM Dim_Month\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_date = pd.read_sql(\"SELECT * FROM Dim_Date\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_salesperson = pd.read_sql(\"SELECT * FROM Dim_SalesPerson\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_territory = pd.read_sql(\"SELECT * FROM Dim_Territory\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_product = pd.read_sql(\"SELECT * FROM Dim_Product\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:26: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_productsubcategory = pd.read_sql(\"SELECT * FROM Dim_ProductSubCategory\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:27: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  dim_productcategory = pd.read_sql(\"SELECT * FROM Dim_ProductCategory\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  fact_salesorder = pd.read_sql(\"SELECT * FROM Fact_SalesOrder\", conn)\n",
      "C:\\Users\\PE\\AppData\\Local\\Temp\\ipykernel_38316\\1385756148.py:29: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  fact_product = pd.read_sql(\"SELECT * FROM Fact_Product\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration de la chaîne de connexion\n",
    "conn_str = (\n",
    "    \"Driver={ODBC Driver 17 for SQL Server};\"  # Remplacez par votre pilote installé\n",
    "    \"Server=DESKTOP-1RQBJ5I;\"  # Remplacez par le nom de votre serveur\n",
    "    \"Database=AVW_DW;\"  # Nom de votre base de données\n",
    "    \"Trusted_Connection=yes;\"  # Utilisation de l'authentification Windows\n",
    ")\n",
    "\n",
    "# Define a function to fetch data from SQL database for all tables\n",
    "def fetch_all_tables_from_sql():\n",
    "    try:\n",
    "        # Établir une connexion à la base de données\n",
    "        with pyodbc.connect(conn_str) as conn:\n",
    "            print(\"Connexion réussie à la base de données AVW_DW !\")\n",
    "            \n",
    "            # Fetch data for each table into pandas DataFrames\n",
    "            dim_year = pd.read_sql(\"SELECT * FROM Dim_Year\", conn)\n",
    "            dim_month = pd.read_sql(\"SELECT * FROM Dim_Month\", conn)\n",
    "            dim_date = pd.read_sql(\"SELECT * FROM Dim_Date\", conn)\n",
    "            dim_salesperson = pd.read_sql(\"SELECT * FROM Dim_SalesPerson\", conn)\n",
    "            dim_territory = pd.read_sql(\"SELECT * FROM Dim_Territory\", conn)\n",
    "            dim_product = pd.read_sql(\"SELECT * FROM Dim_Product\", conn)\n",
    "            dim_productsubcategory = pd.read_sql(\"SELECT * FROM Dim_ProductSubCategory\", conn)\n",
    "            dim_productcategory = pd.read_sql(\"SELECT * FROM Dim_ProductCategory\", conn)\n",
    "            fact_salesorder = pd.read_sql(\"SELECT * FROM Fact_SalesOrder\", conn)\n",
    "            fact_product = pd.read_sql(\"SELECT * FROM Fact_Product\", conn)\n",
    "        \n",
    "            # Return all DataFrames as a dictionary for easy access\n",
    "            return {\n",
    "                \"Dim_Year\": dim_year,\n",
    "                \"Dim_Month\": dim_month,\n",
    "                \"Dim_Date\": dim_date,\n",
    "                \"Dim_SalesPerson\": dim_salesperson,\n",
    "                \"Dim_Territory\": dim_territory,\n",
    "                \"Dim_Product\": dim_product,\n",
    "                \"Dim_ProductSubCategory\": dim_productsubcategory,\n",
    "                \"Dim_ProductCategory\": dim_productcategory,\n",
    "                \"Fact_SalesOrder\": fact_salesorder,\n",
    "                \"Fact_Product\": fact_product,\n",
    "            }\n",
    "    except Exception as e:\n",
    "        print(\"Erreur lors de la connexion ou de la récupération des données :\", e)\n",
    "        return {}\n",
    "\n",
    "# Fetch all tables\n",
    "tables = fetch_all_tables_from_sql()\n",
    "\n",
    "# Example: Print the first few rows of Dim_Year and Fact_SalesOrder tables\n",
    "if tables:  # Ensure tables are loaded before printing\n",
    "    print(tables[\"Dim_Year\"].head())\n",
    "    print(tables[\"Fact_SalesOrder\"].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion des tables\n",
    "\n",
    "Cette section fusionne les différentes tables à l'aide des clés de jointure identifiées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes de Fact_SalesOrder: Index(['Id', 'DateKey', 'TerritoryKey', 'SalePersonKey', 'Revenue',\n",
      "       'NumberOrder'],\n",
      "      dtype='object')\n",
      "Colonnes de Dim_Product: Index(['ProductKey', 'Name', 'ProductNumber', 'StandardCost', 'ListPrice',\n",
      "       'Weight', 'ProductSubCategoryKey', 'ProductId'],\n",
      "      dtype='object')\n",
      "La clé 'ProductKey' n'est pas disponible dans les tables fournies.\n"
     ]
    }
   ],
   "source": [
    "# Afficher les colonnes des DataFrames pour vérifier les clés de jointure disponibles\n",
    "print(\"Colonnes de Fact_SalesOrder:\", tables[\"Fact_SalesOrder\"].columns)\n",
    "print(\"Colonnes de Dim_Product:\", tables[\"Dim_Product\"].columns)\n",
    "\n",
    "# Utilisez les clés correctes pour la jointure\n",
    "merged_df = pd.merge(tables[\"Fact_SalesOrder\"], tables[\"Dim_Date\"], on='DateKey', how='left')\n",
    "merged_df = pd.merge(merged_df, tables[\"Dim_Territory\"], on='TerritoryKey', how='left')\n",
    "\n",
    "# Vérifiez si 'ProductKey' est la bonne clé pour Dim_Product, sinon utilisez la clé correcte\n",
    "if 'ProductKey' in tables[\"Dim_Product\"].columns and 'ProductKey' in tables[\"Fact_SalesOrder\"].columns:\n",
    "    merged_df = pd.merge(merged_df, tables[\"Dim_Product\"], on='ProductKey', how='left')\n",
    "else:\n",
    "    print(\"La clé 'ProductKey' n'est pas disponible dans les tables fournies.\")\n",
    "    # Identifiez et utilisez la bonne clé de jointure ici\n",
    "\n",
    "# Continuez avec les autres jointures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vérification des colonnes des tables\n",
    "\n",
    "Cette section affiche les colonnes disponibles dans chaque table pour identifier les clés de jointure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes de Fact_SalesOrder: Index(['Id', 'DateKey', 'TerritoryKey', 'SalePersonKey', 'Revenue',\n",
      "       'NumberOrder'],\n",
      "      dtype='object')\n",
      "Colonnes de Dim_Product: Index(['ProductKey', 'Name', 'ProductNumber', 'StandardCost', 'ListPrice',\n",
      "       'Weight', 'ProductSubCategoryKey', 'ProductId'],\n",
      "      dtype='object')\n",
      "Colonnes de Dim_Date: Index(['DateKey', 'MonthKey', 'DATE'], dtype='object')\n",
      "Colonnes de Dim_Territory: Index(['TerritoryKey', 'TerritoryId', 'Name', 'ContryRegionCode'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Afficher les colonnes de chaque table\n",
    "print(\"Colonnes de Fact_SalesOrder:\", tables[\"Fact_SalesOrder\"].columns)\n",
    "print(\"Colonnes de Dim_Product:\", tables[\"Dim_Product\"].columns)\n",
    "print(\"Colonnes de Dim_Date:\", tables[\"Dim_Date\"].columns)\n",
    "print(\"Colonnes de Dim_Territory:\", tables[\"Dim_Territory\"].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la colonne 'IsDeleted' dans toutes les tables si elle existe\n",
    "for table_name, table_data in tables.items():\n",
    "    if \"IsDeleted\" in table_data.columns:\n",
    "        tables[table_name].drop(columns=[\"IsDeleted\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'DateKey', 'TerritoryKey', 'ProductKey', 'Qty', 'TerritoryId',\n",
      "       'Name_x', 'ContryRegionCode', 'MonthKey', 'DATE', 'ProductNumber',\n",
      "       'StandardCost', 'ListPrice', 'Weight', 'ProductSubCategoryKey',\n",
      "       'ProductId', 'ProductSubCategoryId', 'Name', 'ProductCategoryKey'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Merge Dim_ProductSubCategory first to ensure ProductCategoryKey is included in merged_df\n",
    "merged_df = pd.merge(merged_df, tables[\"Dim_ProductSubCategory\"], on='ProductSubCategoryKey', how='left')\n",
    "\n",
    "# Verify that ProductCategoryKey is now present\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'DateKey', 'TerritoryKey', 'ProductKey', 'Qty', 'TerritoryId',\n",
      "       'Name_x', 'ContryRegionCode', 'MonthKey', 'DATE', 'ProductNumber',\n",
      "       'StandardCost', 'ListPrice', 'Weight', 'ProductSubCategoryKey',\n",
      "       'ProductId', 'ProductSubCategoryId', 'Name_merged',\n",
      "       'ProductCategoryKey', 'ProductCategoryId', 'Name_category'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Now merge Dim_ProductCategory with the existing merged_df\n",
    "merged_df = pd.merge(merged_df, tables[\"Dim_ProductCategory\"], on='ProductCategoryKey', how='left', suffixes=('_merged', '_category'))\n",
    "\n",
    "# Check the final columns\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportation des données\n",
    "\n",
    "Cette section exporte le DataFrame final fusionné dans un fichier Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "merged_df.to_excel(r\"C:\\Users\\PE\\Desktop\\tesdt.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
